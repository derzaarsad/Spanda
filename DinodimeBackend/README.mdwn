# dinodime CDK Demo

This repository demonstrates the setup of a TypeScript 'monorepo' for dinodime. It contains a subset of the dinodime code ported to TypeScript (handling of notifications on new transactions), and a small AWS deployment driven by the Amazon CDK. The repository aims work out a way to organize code in a modular manner and integrate the CDK into the deployment workflow.

## Repository Organization

The project layout is organized according to the conventions of [lerna][1] and is powered by [Yarn Workspaces][2]. Each module is defined in a "package", which is a standard node module, i.e. it can export an API and has its own dependencies. Because modules are logically isolated, but can dependend on each other, the project requires a tool, which can link modules in such a way that module dependencies are provided in the scope of modules that need them. This interlinking is provided by Yarn Workspaces.

The repository contains the following modules:

- _dinodime-lib_: common classes that are intended to be used as building blocks. This includes the finAPI client, the finAPI data model, controllers that are used in the lambdas, etc.
- _dinodime-lambda_: lambda handlers. The reason why they are extracted in a different module is that this code should be packaged later by webpack and/or rollup, in order to minify the deployment to AWS.
- _dinodime-infrastructure_: infrastructure as code implemented with [Amazon CDK][3].
- _dinodime-notifications-integtest_: an integration test running against the deployed environment.

TypeScript code is compiled incrementally by leveraging [project references][4]. The module dependencies are represented by the `references` array in each `tsconfig*.json` file.

## Deployed Stacks

The CDK app deploys:

- the Mobile App API
- a DynamoDB table containing notification subscriptions
- an API gateway with a single method receiving the notification events from finAPI
- a lambda handler that puts the received notifications on a topic
- a queue subscribed to the topic that stores the notifications for further processing

Refer to the diagram below for a different perspective. Nodes connected via dashed lines are not part of this project.

![The notifications stack](documentation/infrastructure.png)

## Requirements

You need to explicitly install the Yarn and the Amazon CDK binaries.

## Infrastructure Diagrams

### Database Migrations

The migrations infrastructure is responsible for initializing the RDS instance with the schema definitions of the data model. It requires a bootstrapping step that prepares an image with the migrations scripts and pushes it to an internal docker repository. The step can be executed by

```
yarn workspace dinodime-infrastructure run cdk bootstrap -c awsRegion=${AWS_REGION} -c awsAccount=${AWS_ACCOUNT}
```

followed by the [bootstrap-migrations.sh](https://github.com/derzaarsad/Spanda/blob/master/DinodimeBackend/bootstrap-migrations.sh) script.

The diagram below examines the workflow and related resources:

![The database migrations workflow architecture](documentation/database-migrations.png)

1. Once the database is up, a CloudWatch event rule picks it up.
2. The rule executes the database migrations state machine.
3. The state machine contains a single step, which executes the migrations container container.
4. The container connects with the RDS instance and updates the schemas.
5. The logs of the container execution are stored in CloudWatch.

## Development Workflow

Immediately after checkout, you need to do `yarn install`. This downloads all project dependencies and links the dependend modules together.

The project contains a "solution file" that builds everything with a single command: `yarn run build`. You can view the JavaScript output in the `out` directory of each module. While developing, it makes more sense to be performing the build continuously with the "watch" script: `yarn run watch`.

All unit tests from a module can be run by issuing `yarn workspace dinodime-<module_name> run test`.

Lambda deployment packages are created by [webpack][5]. To package the handler, do `yarn workspace dinodime-lambda run package` from the project root.

The deployment can also be triggered directly from the project root, e.g.:

```
yarn workspace dinodime-infrastructure run cdk bootstrap
yarn workspace dinodime-infrastructure run cdk list
yarn workspace dinodime-infrastructure run cdk diff
yarn workspace dinodime-infrastructure run cdk deploy
```

The lines above correspond to a typical sequence of commands you would use when working with the CDK.

## HOWTO

### Connect to a bastion host with the AWS Systems Manager

The infrastructure stack deploys launch templates for bastion hosts inside our VPC. There is a dedicated template for each of our private subnets. This feature requires the [session manager][6] plugin to be installed on your machine.

To connect to a bastion host from your shell, first launch an instance choosing either one of the launch templates. Wait for the instance to come up and note its id. Then, from the command-line do:

    aws ssm start-session --target <instance-id>

Alternatively, you can initiate an SSH session direct from the browser by clicking on the Connect button on the EC2 console and choosing "Session Manager."

[1]: https://lerna.js.org/
[2]: https://yarnpkg.com/lang/en/docs/workspaces/
[3]: https://docs.aws.amazon.com/cdk/
[4]: https://www.typescriptlang.org/docs/handbook/project-references.html
[5]: https://webpack.js.org/
[6]: https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-working-with-install-plugin.html#install-plugin-macos

### Execute integration test locally

Docker is required to execute integration locally. Before executing the docker command, some preparation has to be done to make sure that the FinAPI sandbox contains the needed data. This preparation can be seen in the [azure-pipelines.yml](https://github.com/derzaarsad/Spanda/blob/master/azure-pipelines.yml) at task "integration test preparation".

After the preparation is made, it is recommended to cleanup the existing relevant docker images. The following commands remove **all** existing docker containers and images in the local computer:

```
docker rm -vf $(docker ps -a -q)
docker rmi -f $(docker images -a -q)
```

The integration test can be executed using the following command:

```
docker-compose -f packages/lib/docker-compose.yml build && docker-compose -f packages/lib/docker-compose.yml up --abort-on-container-exit
```
